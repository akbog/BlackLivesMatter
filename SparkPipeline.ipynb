{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Config + Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col, udf\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner, ViveknSentimentApproach, SentimentDetector, SentenceDetector, Stemmer, Lemmatizer\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_JARS\"] = \"../../Urban-Research/venv-urban/lib/python3.7/site-packages/pyspark/jars\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"../../Urban-Research/venv-urban/lib/python3.7/site-packages/pyspark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/local/bin/python3\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/\"\n",
    "os.environ[\"OBJC_DISABLE_INITIALIZE_FORK_SAFETY\"] = \"YES\"\n",
    "# os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk\"\n",
    "# os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x12913aa50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.setAppName(\"LDA_App\")\n",
    "conf.setMaster(\"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext(conf = conf)\n",
    "sqlCtx = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = \"Tweets/streamed_05_06_2020_20_46_18.json.gz\" #Single File\n",
    "input_file = \"TweetsSample\" #Directory\n",
    "twitter = sqlCtx.read.json(input_file)\n",
    "twitter.registerTempTable(\"tweets\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter.where(~col(\"id\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets: 53933\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Tweets:\", twitter.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line to show schema: twitter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To see how Spark is configured and what tasks have been executed, visit the localhost port (here 4040) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persist in Memory\n",
    "# sqlCtx.cacheTable(\"tweets\")\n",
    "#Uncache in Memory\n",
    "# sqlCtx.uncacheTable(\"tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Culling to root-n Retweet Representation in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, to_date as _to_date, sqrt as _sqrt, col, lit, monotonically_increasing_id\n",
    "from datetime import date\n",
    "\n",
    "retweets = twitter.groupBy(\"retweeted_status.id\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "from pyspark.sql import Window\n",
    "w = Window.partitionBy('retweeted_status.id').orderBy('retweeted_status.id')\n",
    "\n",
    "rt_count = twitter.withColumn('mono_id', f.row_number().over(w))\n",
    "root_filt = rt_count.withColumn('rt_count', f.max('mono_id').over(w)).where((f.col('mono_id') <= f.ceil(f.sqrt('rt_count'))) | (f.col('retweeted_status.id').isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|                 id|mono_id|\n",
      "+-------------------+-------+\n",
      "|1268754856501706752|      1|\n",
      "|1268754776474488834|      1|\n",
      "|1268754750612201472|      1|\n",
      "|1268754724615974912|      1|\n",
      "|1268754663983075329|      1|\n",
      "|1268754654176968704|      1|\n",
      "|1268754653392576513|      1|\n",
      "|1268754644567719937|      1|\n",
      "|1268754627123728391|      1|\n",
      "|1268754618613260289|      1|\n",
      "+-------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_filt.select(\"retweeted_status.id\", \"mono_id\").orderBy(col(\"retweeted_status.id\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_filt = root_filt.drop(\"rt_count\").drop(\"mono_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets Remaining:  29380\n"
     ]
    }
   ],
   "source": [
    "root_filt.cache()\n",
    "print(\"Number of Tweets Remaining: \", root_filt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Suspicious Users from Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Bounds\n",
    "stats = twitter.select(\n",
    "                f.mean(f.log10(col('user.followers_count') + 1)).alias('followers_mean'),\n",
    "                f.stddev(f.log10(col('user.followers_count') + 1)).alias('followers_std'),\n",
    "                f.mean(f.log10(col('user.friends_count') + 1)).alias('friends_mean'),\n",
    "                f.stddev(f.log10(col('user.friends_count') + 1)).alias('friends_std'),\n",
    "                f.mean(f.log10(col('user.statuses_count') + 1)).alias('statuses_mean'),\n",
    "                f.stddev(f.log10(col('user.statuses_count') + 1)).alias('statuses_std')\n",
    "                ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_filt = twitter.where((f.log10(f.col('user.followers_count')) <= stats[0]['followers_mean'] + 3*stats[0]['followers_std']) & \\\n",
    "                        (f.log10(f.col('user.friends_count')) <= stats[0]['friends_mean'] + 3*stats[0]['friends_std']) & \\\n",
    "                        (f.log10(f.col('user.statuses_count')) <= stats[0]['statuses_mean'] + 3*stats[0]['statuses_std']) & \\\n",
    "                        (f.log10(f.col('user.followers_count')) >= stats[0]['followers_mean'] - 3*stats[0]['followers_std']) & \\\n",
    "                        (f.log10(f.col('user.friends_count')) >= stats[0]['friends_mean'] - 3*stats[0]['friends_std']) & \\\n",
    "                        (f.log10(f.col('user.statuses_count')) >= stats[0]['statuses_mean'] - 3*stats[0]['statuses_std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets Remaining:  52723\n"
     ]
    }
   ],
   "source": [
    "user_filt.cache()\n",
    "print(\"Number of Tweets Remaining: \", user_filt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersect of Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a very costly operation on large datasets (should only be used as neccesarry)\n",
    "# urt_root = user_filt.intersect(root_filt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In fact, you can just find the lesser of the two, and keep only values in column in present in the next column\n",
    "if user_filt.count() > root_filt.count():\n",
    "    urt_root = user_filt.join(root_filt,[\"id\"], how='leftsemi')\n",
    "else:\n",
    "    urt_root = root_filt.join(user_filt,[\"id\"], how='leftsemi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets Remaining:  28474\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Tweets Remaining: \", urt_root.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpersist\n",
      "persist\n"
     ]
    }
   ],
   "source": [
    "#Clear Cache\n",
    "user_filt.unpersist()\n",
    "root_filt.unpersist()\n",
    "print(\"unpersist\")\n",
    "#Persist New Data\n",
    "urt_root.cache()\n",
    "print(\"persist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beginning Natural Language Processing Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import ViveknSentimentApproach, SentimentDetector, SentenceDetector, Stemmer, Lemmatizer, NGramGenerator, TextMatcher\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to get full_text or text or the original RT Text depending on the type\n",
    "text_ready = urt_root.withColumn('full_tweet_text', f.when(~f.col('retweeted_status.extended_tweet.full_text').isNull(), f.col('retweeted_status.extended_tweet.full_text')) \\\n",
    "                                                .when(~f.col('retweeted_status.text').isNull(), f.col('retweeted_status.text')) \\\n",
    "                                                .when(~f.col('extended_tweet.full_text').isNull(), f.col('extended_tweet.full_text')) \\\n",
    "                                                .otherwise(f.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('full_tweet_text') \\\n",
    "     .setOutputCol('document') \\\n",
    "     .setCleanupMode('shrink')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"token\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "     #.setStopWords([\"no\", \"without\"]) (e.g. read a list of words from a txt)\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(True)\\\n",
    "    .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n",
    "    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
    "    \n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "lemmatizer = Lemmatizer() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"lemma\") \\\n",
    "    .setDictionary(\"AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")\n",
    "\n",
    "bigram = NGramGenerator() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol(\"ngram_2\") \\\n",
    "    .setN(2) \\\n",
    "    .setEnableCumulative(False) \\\n",
    "    .setDelimiter(\" \")\n",
    "\n",
    "trigram = NGramGenerator() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol(\"ngram_3\") \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(False) \\\n",
    "    .setDelimiter(\" \")\n",
    "\n",
    "quadgram = NGramGenerator() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol(\"ngram_4\") \\\n",
    "    .setN(4) \\\n",
    "    .setEnableCumulative(False) \\\n",
    "    .setDelimiter(\" \")\n",
    "    \n",
    "nlpPipeline = Pipeline(stages=[\n",
    " documentAssembler, \n",
    " tokenizer,\n",
    " stopwords_cleaner,\n",
    " normalizer,\n",
    " lemmatizer,\n",
    " bigram,\n",
    " trigram,\n",
    " quadgram\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = documentAssembler.transform(text_ready)\n",
    "# token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
    "# token_df.select('token.result').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty_df = spark.createDataFrame([['']]).toDF(\"full_tweet_text\")\n",
    "\n",
    "pipelineModel = nlpPipeline.fit(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipelineModel.transform(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.withColumn(\"bigrams\", f.col(\"ngram_2.result\"))\n",
    "result = result.withColumn(\"trigrams\", f.col(\"ngram_3.result\"))\n",
    "result = result.withColumn(\"quadgrams\", f.col(\"ngram_4.result\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We need to write code for the n-gram detection\n",
    "##### I say we calculate the co-occurence matrix between terms, and take the top 20% of those terms as the co-occurence dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 is to (in one pass-through), create a vector \n",
    "# (Count Vectorizer of N-grams, taking n-grams that only occur above a certain threshold)\n",
    "bigram_cv = CountVectorizer(inputCol=\"bigrams\", outputCol=\"bigram_feature\", minDF = 0.0008, binary = True)\n",
    "trigram_cv = CountVectorizer(inputCol=\"trigrams\", outputCol=\"trigram_feature\", minDF = 0.0008, binary = True)\n",
    "quadgram_cv = CountVectorizer(inputCol=\"quadgrams\", outputCol=\"quadgram_feature\", minDF = 0.0008, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 is to count co-occurence + populate the matrix\n",
    "bigram_cv_mod = bigram_cv.fit(result)\n",
    "trigram_cv_mod = trigram_cv.fit(result)\n",
    "quadgram_cv_mod = quadgram_cv.fit(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectors = bigram_cv_mod.transform(result)\n",
    "trigram_vectors = trigram_cv_mod.transform(result)\n",
    "quadgram_vectors = quadgram_cv_mod.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "# print(\"Vocab Size: \", ngram_cv_mod.getVocabSize())\n",
    "# print(\"Max Ngram Count: \", ngram_cv_mod.getMaxDF())\n",
    "# print(\"Ngram: \", ngram_cv_mod.getMinDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['racist white', 'pay attention', 'crack skull', 'one black', '8 pm', 'protestor one', 'serious stable', 'say defund', 'day george', 'medic help', 'animal cross', 'many police', 'keep go', 'come back', 'head hit', 'fun fact', 'cop riot', 'floyd sign', 'fuck happen', 'stop kill', 'plan parenthood', 'take time', 'stay home', '8 year', 'trump campaign', 'bill de', 'first place', 'cop buffalo', 'past curfew', 'completely peaceful']\n",
      "['black life matter', 'buffalo police department', 'police department fire', 'department fire police', 'office aaron torgalski', 'police office aaron', 'fire police office', 'aaron torgalski sign', 'torgalski sign petition', 'suspend without pay', 'new york city', 'year old girl', 'protest police brutality', 'shove elderly man', 'death george floyd', '10 year old', 'name jared campbell', 'officer suspend without', 'badge number 8470', 'campbell badge number', 'maced 10 year', 'jared campbell badge', 'report name jared', 'number 8470 spread', '8470 spread info', 'cover badge people', 'de george floyd', 'officer maced 10', 'buffalo police commissioner', 'officer aaron torgalski']\n",
      "['buffalo police department fire', 'police department fire police', 'fire police office aaron', 'police office aaron torgalski', 'department fire police office', 'aaron torgalski sign petition', 'office aaron torgalski sign', 'officer suspend without pay', '10 year old girl', 'campbell badge number 8470', 'jared campbell badge number', 'maced 10 year old', 'name jared campbell badge', 'badge number 8470 spread', 'report name jared campbell', 'number 8470 spread info', 'officer maced 10 year', 'take copypaste post let', 'copypaste post let silence', 'post take copypaste post', 'black life matter movement', 'black life matter protest', 'cover badge people couldnt', 'year old girl cover', 'say black life matter', 'old girl cover badge', 'couldnt report name jared', 'girl cover badge people', 'badge people couldnt report', 'people couldnt report name']\n",
      "Number of Bigrams:  805\n",
      "Number of Trigrams:  143\n",
      "Number of Quadgrams:  71\n"
     ]
    }
   ],
   "source": [
    "#These are great functions to call if you're confused about the functionality of certain parameters\n",
    "# ngram_cv_mod.explainParams()\n",
    "# ngram_cv_mod.extractParamMap()\n",
    "print(bigram_cv_mod.vocabulary[-30:])\n",
    "print(trigram_cv_mod.vocabulary[0:30])\n",
    "print(quadgram_cv_mod.vocabulary[0:30])\n",
    "print(\"Number of Bigrams: \" ,len(bigram_cv_mod.vocabulary))\n",
    "print(\"Number of Trigrams: \" ,len(trigram_cv_mod.vocabulary))\n",
    "print(\"Number of Quadgrams: \" ,len(quadgram_cv_mod.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_list = quadgram_cv_mod.vocabulary + trigram_cv_mod.vocabulary + bigram_cv_mod.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputting bigrams, trigrams, and quadrgams\n",
    "# with open('bigrams.txt', 'w') as fbi:\n",
    "#     for item in bigram_cv_mod.vocabulary:\n",
    "#         fbi.write(\"%s\\n\" % item)\n",
    "\n",
    "# with open('trigrams.txt', 'w') as ftri:\n",
    "#     for item in trigram_cv_mod.vocabulary:\n",
    "#         ftri.write(\"%s\\n\" % item)\n",
    "        \n",
    "# with open('quadgrams.txt', 'w') as fquad:\n",
    "#     for item in quadgram_cv_mod.vocabulary:\n",
    "#         fquad.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function to compare join vocabulary\n",
    "# First build list of n-grams from the n-gram to compare with others\n",
    "# Sample Data Structure\n",
    "# [((0), ['police_act']), ((1), ['justice_emerald']), ..., ((10), ['dc_protest'])]\n",
    "# [((0,1), ['police_act', 'justice_emerald']), ((1, 2), ['justice_emerald', 'life_take']), ...]\n",
    "# in order to build this, you need to remember that \n",
    "# Orrrr....you just go backwards - you replace backwards...s.t. the replacements will be done without worry...\n",
    "# Clearly the superior solution ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ngrams.txt', 'w') as fout:\n",
    "    for item in pre_ngrams:\n",
    "        fout.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NgramReplacementDuhhhhhhhhh import NGramReplacer\n",
    "from sparknlp.base import TokenAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('full_tweet_text') \\\n",
    "     .setOutputCol('document') \\\n",
    "     .setCleanupMode('shrink')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"token\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "     #.setStopWords([\"no\", \"without\"]) (e.g. read a list of words from a txt)\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"normalized\")\\\n",
    "    .setLowercase(True)\\\n",
    "    .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # remove punctuations (keep alphanumeric chars)\n",
    "    # if we don't set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])\n",
    "    \n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "lemmatizer = Lemmatizer() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"lemma\") \\\n",
    "    .setDictionary(\"AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCols(\"finished\") \\\n",
    "    .setCleanAnnotations(True) \\\n",
    "    .setIncludeMetadata(False)\n",
    "    \n",
    "nlpPipeline = Pipeline(stages=[\n",
    " documentAssembler, \n",
    " tokenizer,\n",
    " stopwords_cleaner,\n",
    " normalizer,\n",
    " lemmatizer,\n",
    " finisher\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = documentAssembler.transform(text_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel = nlpPipeline.fit(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipelineModel.transform(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fin = result.withColumn(\"final\", f.concat_ws(\" \", \"finished\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('final') \\\n",
    "     .setOutputCol('document') \\\n",
    "     .setCleanupMode('shrink')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setExceptions(large_list)\n",
    "\n",
    "# finisher = Finisher() \\\n",
    "#     .setInputCols([\"token\"]) \\\n",
    "#     .setOutputCols(\"finished\") \\\n",
    "#     .setCleanAnnotations(True) \\\n",
    "#     .setIncludeMetadata(False)\n",
    "\n",
    "nlpPipeline = Pipeline(stages = [\n",
    "    documentAssembler,\n",
    "    tokenizer\n",
    "#     finisher\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModelNgram = nlpPipeline.fit(result_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ngram = pipelineModelNgram.transform(result_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                         lda_input|\n",
      "+--------------------------------------------------+\n",
      "|[juggernautbg, funny, part, clip, every single,...|\n",
      "|[seattle, lift, curfew, cop havent, tear gass, ...|\n",
      "|                   [twitter, unite, hurt, elderly]|\n",
      "|[fact, month ago, many, centrist, dem, push, gu...|\n",
      "|                                     [hear, story]|\n",
      "|[cop dont need well, pay, teacher, healthcare, ...|\n",
      "|[even, individual, one, make, extremely, racist...|\n",
      "|[usually, save, fuck, guy, segment, podcast, of...|\n",
      "|[it, personal, mission, white, minnesotans, ame...|\n",
      "|[shock, video, lapd officer see, strike, protes...|\n",
      "|[281, nyc, notice, tactic, kettle, protestor, 8...|\n",
      "|[president, aka, chicken, little, literally, wa...|\n",
      "|[many video police, violently, attack, peaceful...|\n",
      "|[white house, look like, prison, httpstcofyoc4t...|\n",
      "|[see, speechless, get, everyone know right poli...|\n",
      "|[protestor, yell, police, you, ancestor, proud,...|\n",
      "|                                                []|\n",
      "|[condemn, release, student, protest, widely, cr...|\n",
      "|[police, harassment, blackfriday, protest, cebu...|\n",
      "|                      [know right protest, thread]|\n",
      "+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result_ngram.select(\"matched_ngrams.result\", \"token.result\").show(100, truncate = 50)\n",
    "lda_df = result_ngram.withColumn(\"lda_input\", f.col(\"token.result\"))\n",
    "lda_df.select(\"lda_input\").show(20, truncate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_fin = result_ngram.withColumn(\"final\", f.concat_ws(\" \", \"token.result\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark.sql.types as Types\n",
    "\n",
    "# def get_sent(x):\n",
    "#     import textblob\n",
    "#     res = textblob.TextBlob(x).sentiment[0]\n",
    "#     return res\n",
    "\n",
    "# sentiment = f.udf(lambda x : get_sent(x), Types.DoubleType())\n",
    "\n",
    "# tweets = result_fin.withColumn('sentiment',sentiment(f.col(\"final\")))\n",
    "\n",
    "# TextBlob(\"My name is alexander and I'm very hungry\").sentiment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try with small sample\n",
    "# result_sample = result_fin.sample(False, 0.01)\n",
    "# tweets = result_sample.withColumn('sentiment',sentiment(f.col(\"final\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to get dtype\n",
    "# def get_dtype(df,colname):\n",
    "#     return [dtype for name, dtype in df.dtypes if name == colname][0]\n",
    "\n",
    "# get_dtype(result_fin,'final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What's left? Attach LDA to the end (There is some preprocessing work left but for now, this will work to write)\n",
    "#Operating on subset of actual data\n",
    "lda_sample = lda_df.sample(False, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size:  7074\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Size: \", lda_sample.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Persist\n",
    "lda_sample.cache()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Vector Representation of the dataset\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "# Optional Parameter:  vocabSize = 10000\n",
    "lda_vect = CountVectorizer(inputCol=\"lda_input\", outputCol=\"features\", maxDF = 0.5, minDF = 0.0008, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_vect_mod = lda_vect.fit(lda_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  2655\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab Size: \", len(lda_vect_mod.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.CountVectorizerModel"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda_vect_mod.save(\"lda_vectorizer_model\")\n",
    "type(lda_vect_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_vect_mod = CountVectorizer.load(\"lda_vectorizer_model\")\n",
    "lda_vect_mod = lda_vect_mod.setInputCol(\"lda_input\")\n",
    "lda_vect_mod = lda_vect_mod.setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_features = lda_vect_mod.transform(lda_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDA_177d0501b9ef"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(k = 40, optimizer = \"online\")\n",
    "lda.setMaxIter(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = lda.fit(lda_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = lda_model.logLikelihood(lda_features)\n",
    "lp = lda_model.logPerplexity(lda_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -541590.1134235631\n",
      "The upper bound on perplexity: 8.521329097087072\n"
     ]
    }
   ],
   "source": [
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.describeTopics(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-------------------------------------------------------------------+\n",
      "|topic|termIndices       |termWeights                                                        |\n",
      "+-----+------------------+-------------------------------------------------------------------+\n",
      "|0    |[1571, 1890, 1112]|[0.007253373679139064, 5.028304972047716E-4, 4.7570577365756443E-4]|\n",
      "|1    |[744, 2441, 2550] |[0.03221213654352879, 0.0224194382642429, 0.019918551127648416]    |\n",
      "|2    |[3, 42, 11]       |[0.0344381894702296, 0.011558164734669207, 0.010580260573407052]   |\n",
      "|3    |[483, 147, 951]   |[0.029878000482313927, 0.027850941489825736, 0.025493516560262445] |\n",
      "|4    |[89, 357, 1012]   |[0.0334416545991758, 0.03146388405278239, 0.02693706636790752]     |\n",
      "|5    |[113, 73, 276]    |[0.039375281987895465, 0.0369442358746812, 0.022985886225573945]   |\n",
      "|6    |[1455, 826, 52]   |[0.028197715863123397, 0.025665659429518744, 0.023303554586465786] |\n",
      "|7    |[2216, 159, 484]  |[6.238828173157206E-4, 5.855936643914951E-4, 5.72263912824255E-4]  |\n",
      "|8    |[468, 707, 519]   |[0.036606163781539656, 0.02403842893186963, 0.023185479630628635]  |\n",
      "|9    |[907, 890, 493]   |[0.04733648548689997, 0.04204416468418675, 0.04137238439558164]    |\n",
      "|10   |[315, 1385, 1493] |[0.05992097395831555, 0.022013067292553277, 0.020524862735135766]  |\n",
      "|11   |[247, 94, 1099]   |[0.029483850057804642, 0.024933681811288206, 0.024019912585707834] |\n",
      "|12   |[141, 976, 49]    |[9.089971829581099E-4, 7.569132524112346E-4, 7.528958034224751E-4] |\n",
      "|13   |[1635, 875, 637]  |[0.03387205293652508, 0.027705070886009552, 0.023794508895812007]  |\n",
      "|14   |[2598, 1709, 733] |[0.02413715365103181, 0.018527428861927465, 0.01646837407380042]   |\n",
      "|15   |[685, 374, 2636]  |[0.018736667048536854, 0.0178834873341041, 0.01383983590639573]    |\n",
      "|16   |[887, 205, 170]   |[0.05123778656875424, 0.04339967499968029, 0.039901944858292494]   |\n",
      "|17   |[3, 267, 44]      |[0.02865731297361864, 0.01387690209404685, 0.0136203949594303]     |\n",
      "|18   |[96, 529, 1615]   |[0.034433527644653704, 0.02306779308182514, 0.021778208327236544]  |\n",
      "|19   |[12, 919, 526]    |[0.047704479926335606, 0.03928507679728168, 0.02482008359985737]   |\n",
      "+-----+------------------+-------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = lda_model.transform(lda_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed.select(\"topicDistribution\").show(truncate=True)\n",
    "# transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed:  False\n",
      "Vocab Size:  2655\n"
     ]
    }
   ],
   "source": [
    "print(\"Distributed: \", lda_model.isDistributed())\n",
    "print(\"Vocab Size: \", lda_model.vocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[1571, 1890, 1112...|[0.00725337367913...|\n",
      "|    1|[744, 2441, 2550,...|[0.03221213654352...|\n",
      "|    2|[3, 42, 11, 7, 0,...|[0.03443818947022...|\n",
      "|    3|[483, 147, 951, 1...|[0.02987800048231...|\n",
      "|    4|[89, 357, 1012, 1...|[0.03344165459917...|\n",
      "|    5|[113, 73, 276, 18...|[0.03937528198789...|\n",
      "|    6|[1455, 826, 52, 6...|[0.02819771586312...|\n",
      "|    7|[2216, 159, 484, ...|[6.23882817315720...|\n",
      "|    8|[468, 707, 519, 4...|[0.03660616378153...|\n",
      "|    9|[907, 890, 493, 1...|[0.04733648548689...|\n",
      "|   10|[315, 1385, 1493,...|[0.05992097395831...|\n",
      "|   11|[247, 94, 1099, 1...|[0.02948385005780...|\n",
      "|   12|[141, 976, 49, 19...|[9.08997182958109...|\n",
      "|   13|[1635, 875, 637, ...|[0.03387205293652...|\n",
      "|   14|[2598, 1709, 733,...|[0.02413715365103...|\n",
      "|   15|[685, 374, 2636, ...|[0.01873666704853...|\n",
      "|   16|[887, 205, 170, 1...|[0.05123778656875...|\n",
      "|   17|[3, 267, 44, 45, ...|[0.02865731297361...|\n",
      "|   18|[96, 529, 1615, 1...|[0.03443352764465...|\n",
      "|   19|[12, 919, 526, 17...|[0.04770447992633...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model.describeTopics().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordNumbers = 5  \n",
    "topicIndices = lda_model.describeTopics(maxTermsPerTopic = wordNumbers)\n",
    "vocabArray = lda_vect_mod.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIndices = topicIndices.select(\"termIndices\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delhi\n",
      "cop bad\n",
      "cost\n",
      "btw\n",
      "office\n",
      "['delhi', 'cop bad', 'cost', 'btw', 'office']\n"
     ]
    }
   ],
   "source": [
    "print(topic_render(topicIndices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_render(topic):\n",
    "    terms = topic[0]\n",
    "    result = []\n",
    "    for i in range(wordNumbers):\n",
    "        term = vocabArray[terms[i]]\n",
    "        result.append(term)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(topicIndices):\n",
    "    for topic in range(len(topicIndices)):\n",
    "        print (\"Topic \" + str(topic) + \":\")\n",
    "        print(topic_render(topicIndices[topic]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_topics(topicIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done with First Attempt at Apache Spark LDA Pipeline\n",
    "#Let's make a work schedule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-urban",
   "language": "python",
   "name": "venv-urban"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
